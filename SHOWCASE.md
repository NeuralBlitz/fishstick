# ğŸŸ fishstick

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red?style=for-the-badge&logo=pytorch&logoColor=white" />
  <img src="https://img.shields.io/badge/tests-19%2F19%20passing-green?style=for-the-badge" />
  <img src="https://img.shields.io/badge/LLM%20Models-29%2B-yellow?style=for-the-badge" />
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" />
</p>

<p align="center">
  <b>A Mathematically Rigorous, Physically Grounded AI Framework</b><br>
  <i>Synthesizing Theoretical Physics Ã— Formal Mathematics Ã— Advanced ML</i>
</p>

---

## ğŸ¯ What is fishstick?

**fishstick** is not just another deep learning frameworkâ€”it's a **paradigm shift** in how we approach artificial intelligence. Instead of treating AI as empirical engineering, fishstick treats it as a branch of **mathematical physics**.

### ğŸŒŸ Core Philosophy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "Neural architectures are morphisms in dagger compact      â”‚
â”‚   closed categories; training dynamics are gradient flows   â”‚
â”‚   on statistical manifolds."                                â”‚
â”‚                                                              â”‚
â”‚                           â€” fishstick Manifesto             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Key Features

### 1ï¸âƒ£ Six Unified Frameworks (A-F)

Each framework represents a unique synthesis of mathematical disciplines:

| Framework | Focus | Key Innovation | Parameters |
|-----------|-------|----------------|------------|
| **UniIntelli** (A) | Category + Geometry + Thermodynamics | Holomorphic RG flows with temperature scheduling | 1.8M |
| **HSCA** (B) | Symplectic + Sheaf | Hamiltonian Neural Networks with energy conservation | 6.5M |
| **UIA** (C) | Categorical-Hamiltonian | CHNP with RG-aware autoencoders | 1.7M |
| **SCIF** (D) | Symplectic-Categorical | Fiber bundle geometry + Hamiltonian dynamics | 3.8M |
| **UIF** (E) | Four-Layer Stack | Category â†’ Geometry â†’ Dynamics â†’ Verification | 367K |
| **UIS** (F) | Complete Synthesis | Quantum-inspired + RG + Neuro-symbolic | 861K |

```python
# Use any framework in 3 lines
from fishstick.frameworks.uniintelli import create_uniintelli

model = create_uniintelli(input_dim=784, output_dim=10)
output = model(torch.randn(4, 784))  # torch.Size([4, 10])
```

### 2ï¸âƒ£ Advanced Neural Architectures

#### ğŸ”® Neural ODEs (Continuous-Depth Models)
- **ODEFunction**: Learnable continuous dynamics
- **NeuralODE**: Adaptive solvers (Dormand-Prince, RK4)
- **AugmentedNeuralODE**: Higher-dimensional state spaces
- **LatentODE**: Time series modeling
- **ContinuousNormalizingFlow**: FFJORD-based flows

```python
from fishstick.neural_ode import NeuralODE, ODEFunction

# Continuous-depth model
odefunc = ODEFunction(dim=10, hidden_dim=64)
node = NeuralODE(odefunc, t_span=(0.0, 1.0), method='dopri5')

z0 = torch.randn(4, 10)
z1 = node(z0)  # Integrates through learned dynamics
```

#### ğŸ•¸ï¸ Geometric Graph Neural Networks
- **E(n)-Equivariant** message passing
- **Sheaf structure** for edge/node features  
- **Molecular & crystalline** material support
- 3D coordinate equivariance

```python
from fishstick.graph import EquivariantMessagePassing

layer = EquivariantMessagePassing(node_dim=64, edge_dim=0)
x_out, pos_out = layer(x, pos, edge_index)  # SE(3)-equivariant
```

#### ğŸ² Probabilistic & Bayesian Deep Learning
- **BayesianLinear**: Variational inference for weights
- **DeepEnsemble**: Uncertainty quantification
- **EvidentialLayer**: Normal-Inverse-Gamma parameters
- **ConformalPredictor**: Guaranteed coverage intervals

```python
from fishstick.probabilistic import BayesianNeuralNetwork

bnn = BayesianNeuralNetwork(input_dim=784, hidden_dims=[256, 128])
mean, uncertainty = bnn.predict_with_uncertainty(x, n_samples=100)
```

#### ğŸŒŠ Normalizing Flows
- **RealNVP**: Coupling-based flows
- **Glow**: 1Ã—1 convolutions + act norm
- **MAF**: Masked autoregressive flows
- **Conditional flows**: For conditional generation

```python
from fishstick.flows import RealNVP, Glow, MAF

flow = RealNVP(dim=8, n_coupling=8)
log_prob = flow.log_prob(x)      # Density estimation
samples = flow.sample(1000)       # Generate samples
```

#### âš›ï¸ Equivariant Networks (SE(3), E(3))
- **SE3EquivariantLayer**: 3D rotation/translation equivariant
- **SE3Transformer**: Equivariant attention
- **MolecularEnergy**: Force field prediction
- **Point cloud processing**

```python
from fishstick.equivariant import SE3EquivariantLayer

layer = SE3EquivariantLayer(in_features=32, out_features=32)
features_out, coords_out = layer(features, coords, edge_index)
```

#### ğŸ”— Causal Inference
- **Structural Causal Models** (SCMs)
- **Do-calculus** for interventions
- **Counterfactual reasoning**
- **Causal discovery** (PC algorithm, NOTEARS)
- **Instrumental variables** estimation

```python
from fishstick.causal import StructuralCausalModel, CausalGraph

# Define causal DAG
graph = CausalGraph(n_nodes=3, adjacency=adjacency)
scm = StructuralCausalModel(graph)

# Do-operator: P(Y | do(X=x))
sample = scm.do_calculus(intervention_node=0, value=torch.tensor([[2.0]]))
```

### 3ï¸âƒ£ 29+ Pre-trained LLM Models

Your environment comes with **29 state-of-the-art language models** ready to use:

<details>
<summary><b>ğŸ“ Text Generation (7 models)</b></summary>

- `gpt2` (124M) - The classic
- `gpt2-medium` (345M) - Better quality
- `distilgpt2` (82M) - Fast & efficient
- `EleutherAI/gpt-neo-125M` - Open source
- `EleutherAI/pythia-160m` - Interpretable
- `gpt2-xl` (1.5B) - Maximum quality
- `sshleifer/tiny-gpt2` - Testing

</details>

<details>
<summary><b>ğŸ” Understanding & BERT (8 models)</b></summary>

- `bert-base-uncased/cased` (110M)
- `distilbert-base-uncased` (66M) - 40% smaller, 97% performance
- `roberta-base` (125M)
- `albert-base-v2` (12M)
- `distilroberta-base` (82M)
- `prajjwal1/bert-tiny` (4M) - Ultra fast
- `prajjwal1/bert-mini` (11M)

</details>

<details>
<summary><b>ğŸ¯ Task-Specific Models (14 models)</b></summary>

**Classification:**
- Sentiment analysis
- Named Entity Recognition (2 models)

**Question Answering:**
- DistilBERT SQuAD
- BERT Large SQuAD

**Embeddings:**
- all-MiniLM-L6-v2 (22M) - Best for similarity
- all-distilroberta-v1 (82M)

**Code:**
- CodeGPT Python
- CodeBERT (125M)
- CodeT5 (60M)

**Multilingual:**
- mBERT (104 languages)
- DistilBERT multilingual

**Summarization:**
- DistilBART CNN
- BART Large CNN

</details>

```python
# Instant access to any model
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("distilgpt2")
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

# Generate text
prompt = "The future of AI is"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=30)
```

### 4ï¸âƒ£ Mathematical Foundations

fishstick is built on rigorous mathematical theory:

#### ğŸ“ Information Geometry
- **Statistical Manifolds**: Parameters as points on curved spaces
- **Fisher Information Metric**: Natural gradient descent = geodesic flow
- **Natural Gradient**: Gradient flow w.r.t. Levi-Civita connection

```python
from fishstick import StatisticalManifold, FisherInformationMetric

manifold = StatisticalManifold(dim=10)
metric = manifold.fisher_information(params, log_prob)
```

#### ğŸ§® Category Theory
- **MonoidalCategories**: Tensor product structures
- **Functors & Natural Transformations**: Structure-preserving maps
- **Dagger Categories**: Adjoint operations for learning
- **Lenses**: Bidirectional learners

```python
from fishstick import MonoidalCategory, DaggerCategory, Lens

cat = MonoidalCategory("NeuralCategory")
dagger = DaggerCategory("QuantumData")
lens = Lens(get=lambda x: x * 2, put=lambda s, a: s + a)
```

#### ğŸ”¬ Symplectic Geometry
- **Hamiltonian Neural Networks**: Energy-conserving dynamics
- **Symplectic Integrators**: Leapfrog, Verlet methods
- **Phase Space**: (q, p) coordinates with conservation laws

```python
from fishstick import HamiltonianNeuralNetwork

hnn = HamiltonianNeuralNetwork(input_dim=10, hidden_dim=64)
trajectory = hnn.integrate(z0, n_steps=100, dt=0.01)
# Energy conserved: H(zâ‚€) â‰ˆ H(zâ‚â‚€â‚€)
```

#### ğŸ“Š Sheaf Theory
- **Data Sheaves**: Local-to-global consistency
- **Sheaf Cohomology**: HÂ¹ measures inconsistency
- **Sheaf-Optimized Attention**: Attention with cohomological constraints

```python
from fishstick import DataSheaf, SheafOptimizedAttention

sheaf = DataSheaf(stalk_dim=16, base_space_dim=10)
attn = SheafOptimizedAttention(embed_dim=256)
```

#### ğŸŒ¡ï¸ Thermodynamics
- **Free Energy**: F = U - TS decomposition
- **Thermodynamic Gradient Flow**: Jarzynski equality for training
- **Landauer Bound**: kT ln(2) minimum energy per bit
- **Jarzynski Training**: Non-equilibrium work relations

```python
from fishstick import ThermodynamicGradientFlow, FreeEnergy

tgf = ThermodynamicGradientFlow(lr=0.01, temperature=1.0, friction=0.1)
free_energy = FreeEnergy(likelihood_fn, beta=1.0)
```

#### âš›ï¸ Renormalization Group
- **RG Flows**: Coarse-graining across scales
- **Fixed Point Detection**: Critical behavior analysis
- **Universality Classes**: Pattern classification
- **RG-Aware Autoencoders**: Multi-scale representations

```python
from fishstick import RGAutoencoder, RGFlow

rg_ae = RGAutoencoder(input_dim=784, latent_dims=[128, 64, 32])
rw_flow = RGFlow(n_scales=4)
```

#### âœ… Formal Verification
- **Dependent Types**: Curry-Howard correspondence
- **Lipschitz Constants**: Robustness bounds
- **SMT Verification**: Satisfiability modulo theories
- **Verification Certificates**: Provable correctness

```python
from fishstick import DependentlyTypedLearner, VerificationPipeline

learner = DependentlyTypedLearner(input_dim=10, output_dim=1)
certificate = learner.verify_property("Lipschitz", epsilon=0.1)
```

## ğŸ“Š Performance Benchmarks

All frameworks tested on synthetic data with **100% test pass rate**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Framework    â”‚ Parameters â”‚ Forward Passâ”‚ Energy Cons. â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ UniIntelli     â”‚   1.8M     â”‚     âœ“       â”‚      âœ“       â”‚
â”‚ HSCA           â”‚   6.5M     â”‚     âœ“       â”‚      âœ“       â”‚
â”‚ UIA            â”‚   1.7M     â”‚     âœ“       â”‚      âœ“       â”‚
â”‚ SCIF           â”‚   3.8M     â”‚     âœ“       â”‚      âœ“       â”‚
â”‚ UIF            â”‚  367K      â”‚     âœ“       â”‚      âœ“       â”‚
â”‚ UIS            â”‚  861K      â”‚     âœ“       â”‚      âœ“       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Coverage

- **Core Framework**: 13/13 tests passed âœ…
- **Advanced Features**: 6/6 tests passed âœ…
- **LLM Integration**: 29 models ready âœ…
- **Total**: 19/19 tests passed âœ…

## ğŸ¨ Code Examples

### Example 1: Complete Pipeline with fishstick

```python
import torch
from fishstick import HamiltonianNeuralNetwork
from fishstick.frameworks.uniintelli import create_uniintelli
from fishstick.probabilistic import BayesianNeuralNetwork

# 1. Train with energy-conserving dynamics
hnn = HamiltonianNeuralNetwork(input_dim=10, hidden_dim=64)
trajectory = hnn.integrate(z0, n_steps=100)

# 2. Use unified framework for classification
model = create_uniintelli(input_dim=784, output_dim=10)
output = model(images)

# 3. Add uncertainty quantification
bnn = BayesianNeuralNetwork(input_dim=10, hidden_dims=[64])
mean, uncertainty = bnn.predict_with_uncertainty(data)
```

### Example 2: Geometric Deep Learning

```python
from fishstick.graph import EquivariantMessagePassing
from fishstick.equivariant import SE3EquivariantLayer

# Process 3D molecular structures
graph_layer = EquivariantMessagePassing(node_dim=64, edge_dim=0)
se3_layer = SE3EquivariantLayer(in_features=32, out_features=32)

# Both respect geometric symmetries
x_out, pos_out = graph_layer(x, pos, edge_index)
f_out, c_out = se3_layer(features, coords, edge_index)
```

### Example 3: Causal Reasoning

```python
from fishstick.causal import StructuralCausalModel, CausalDiscovery
import numpy as np

# Learn causal structure from data
data = np.random.randn(1000, 5)
adjacency = CausalDiscovery.notears(data)

# Build and use SCM
graph = CausalGraph(n_nodes=5, adjacency=adjacency)
scm = StructuralCausalModel(graph)

# Interventional reasoning
counterfactual = scm.do_calculus(intervention_node=0, value=torch.tensor([[2.0]]))
```

### Example 4: LLM + fishstick Integration

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from fishstick import SheafOptimizedAttention

# Load LLM
llm = AutoModelForCausalLM.from_pretrained("distilgpt2")
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")

# Enhance with fishstick components
sheaf_attn = SheafOptimizedAttention(embed_dim=768, num_heads=12)

# Combine for advanced reasoning
# ... custom architecture combining LLM + fishstick layers
```

## ğŸ› ï¸ Installation

### Quick Start

```bash
# Clone the repository
git clone https://github.com/NeuralBlitz/fishstick.git
cd fishstick

# Install dependencies
pip install torch numpy scipy pyyaml

# Install optional dependencies for full features
pip install torchdiffeq torch-geometric

# Run tests to verify installation
python test_all.py
python test_advanced.py
```

### Verify Installation

```python
import fishstick

# Test basic functionality
from fishstick import HamiltonianNeuralNetwork
model = HamiltonianNeuralNetwork(input_dim=10)
print("âœ“ fishstick installed successfully!")
```

## ğŸ“š Documentation

- **[README.md](README.md)** - Overview and getting started
- **[INSTALLED_MODELS.md](INSTALLED_MODELS.md)** - Complete LLM catalog
- **[LLM_GUIDE.md](LLM_GUIDE.md)** - LLM usage guide
- **[A.md - F.md]** - Mathematical documentation for each framework

### Key Theorems Implemented

1. **Natural Gradient = Geodesic Flow** (Theorem 2.1)
2. **SOA Conserves Sheaf Cohomology** (Theorem 4.1)  
3. **TGF Convergence** (Theorem 5.1)
4. **No-Cloning in Learning** (Lemma 2.1)

## ğŸ¯ Use Cases

### ğŸ”¬ Scientific Computing
- Molecular dynamics with energy conservation
- Material property prediction
- Physics-informed neural networks

### ğŸ§  Neuro-Symbolic AI
- Combining deep learning with formal reasoning
- Causal inference from observational data
- Verified decision-making systems

### ğŸŒ Graph Learning
- 3D molecule generation
- Social network analysis
- Knowledge graph completion

### ğŸ² Uncertainty Quantification
- Risk-aware decision making
- Bayesian optimization
- Safety-critical AI systems

### ğŸ“ Natural Language Processing
- Text generation with 29+ models
- Sentiment analysis
- Question answering
- Code generation

## ğŸ¤ Contributing

We welcome contributions! Areas of interest:

- ğŸ¯ Higher sheaf cohomology (HÂ²) for mode connectivity
- âš›ï¸ Quantum-categorical neural networks  
- âœ… Real-time formal verification
- ğŸ“Š Unified scaling laws from RG fixed points
- ğŸ§ª New scientific applications

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“– Citation

If you use fishstick in your research, please cite:

```bibtex
@software{fishstick_2026,
  title={fishstick: A Mathematically Rigorous AI Framework},
  author={NeuralBlitz},
  url={https://github.com/NeuralBlitz/fishstick},
  year={2026}
}
```

## ğŸ™ Acknowledgments

fishstick synthesizes insights from:

- **Statistical Mechanics** - Thermodynamics of computation
- **Differential Geometry** - Information geometry & manifolds
- **Category Theory** - Compositionality & functorial semantics
- **Algebraic Topology** - Sheaf theory & cohomology
- **Symplectic Geometry** - Hamiltonian dynamics
- **Type Theory** - Formal verification & Curry-Howard

---

<p align="center">
  <b>The era of black-box AI is ending.</b><br>
  <b>The era of principled intelligence begins now.</b><br>
  <br>
  ğŸŸ <b>fishstick</b> â€” Where Mathematics Meets Intelligence
</p>

---

<p align="center">
  <a href="https://github.com/NeuralBlitz/fishstick">â­ Star us on GitHub</a> â€¢
  <a href="https://github.com/NeuralBlitz/fishstick/issues">ğŸ› Report Issues</a> â€¢
  <a href="https://github.com/NeuralBlitz/fishstick/discussions">ğŸ’¬ Discuss</a>
</p>