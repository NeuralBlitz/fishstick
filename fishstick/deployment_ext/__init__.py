"""
fishstick Deployment Extensions

Comprehensive model deployment and serving utilities for the fishstick framework.
Provides ONNX export, TorchScript conversion, quantization, inference optimization,
serving utilities, and benchmarking tools.

Example:
    >>> from fishstick.deployment_ext import ONNXExporter, TorchScriptConverter
    >>> from fishstick.deployment_ext import quantize_model, benchmark_model
"""

from fishstick.deployment_ext.onnx_exporter import (
    ONNXExporter,
    ONNXExportConfig,
    ONNXRuntimeConfig,
    export_to_onnx,
    create_dynamic_quantized_onnx,
    benchmark_onnx_model,
    compare_onnx_models,
    verify_onnx_output,
    ONNXOpsetVersion,
    OptimizationLevel,
)

from fishstick.deployment_ext.torchscript_converter import (
    TorchScriptConverter,
    TorchScriptConfig,
    TorchScriptTracer,
    TorchScriptScripter,
    trace_model,
    script_model,
    load_torchscript,
    benchmark_torchscript,
    verify_torchscript_output,
    ConversionMode,
    OptimizationLevel,
)

from fishstick.deployment_ext.quantizer import (
    DynamicQuantizer,
    StaticQuantizer,
    QATQuantizer,
    PTQQuantizer,
    ModelQuantizer,
    QuantizationResult,
    quantize_dynamic,
    quantize_static,
    quantize_model,
    get_quantization_stats,
    QuantizationType,
    QuantizationDtype,
    QuantizationConfig,
    CalibrationConfig,
)

from fishstick.deployment_ext.inference_optimizer import (
    InferenceOptimizer,
    InferenceConfig,
    GraphOptimizer,
    OperatorFusion,
    MemoryOptimizer,
    CachedInference,
    BatchInferenceOptimizer,
    optimize_for_inference,
    benchmark_inference,
    apply_memory_optimizations,
    OptimizationPass,
)

from fishstick.deployment_ext.serving_utils import (
    ModelLoader,
    ModelWarmer,
    InferenceServer,
    ImagePreprocessor,
    TextPreprocessor,
    AudioPreprocessor,
    ClassificationPostprocessor,
    load_model,
    warmup_model,
    create_preprocessor,
    create_postprocessor,
    ModelFormat,
    ModelMetadata,
    InferenceRequest,
    InferenceResponse,
)

from fishstick.deployment_ext.model_benchmark import (
    ModelBenchmarker,
    LatencyBenchmark,
    ThroughputBenchmark,
    MemoryProfiler,
    AccuracyValidator,
    ModelComparator,
    benchmark_model,
    compare_models,
    BenchmarkConfig,
    BenchmarkResult,
    ComparisonResult,
    BenchmarkMode,
)

__version__ = "0.1.0"

__all__ = [
    "ONNXExporter",
    "ONNXExportConfig",
    "ONNXRuntimeConfig",
    "export_to_onnx",
    "create_dynamic_quantized_onnx",
    "benchmark_onnx_model",
    "compare_onnx_models",
    "verify_onnx_output",
    "ONNXOpsetVersion",
    "OptimizationLevel",
    "TorchScriptConverter",
    "TorchScriptConfig",
    "TorchScriptTracer",
    "TorchScriptScripter",
    "trace_model",
    "script_model",
    "load_torchscript",
    "benchmark_torchscript",
    "verify_torchscript_output",
    "ConversionMode",
    "DynamicQuantizer",
    "StaticQuantizer",
    "QATQuantizer",
    "PTQQuantizer",
    "ModelQuantizer",
    "QuantizationResult",
    "quantize_dynamic",
    "quantize_static",
    "quantize_model",
    "get_quantization_stats",
    "QuantizationType",
    "QuantizationDtype",
    "QuantizationConfig",
    "CalibrationConfig",
    "InferenceOptimizer",
    "InferenceConfig",
    "GraphOptimizer",
    "OperatorFusion",
    "MemoryOptimizer",
    "CachedInference",
    "BatchInferenceOptimizer",
    "optimize_for_inference",
    "benchmark_inference",
    "apply_memory_optimizations",
    "OptimizationPass",
    "ModelLoader",
    "ModelWarmer",
    "InferenceServer",
    "ImagePreprocessor",
    "TextPreprocessor",
    "AudioPreprocessor",
    "ClassificationPostprocessor",
    "load_model",
    "warmup_model",
    "create_preprocessor",
    "create_postprocessor",
    "ModelFormat",
    "ModelMetadata",
    "InferenceRequest",
    "InferenceResponse",
    "ModelBenchmarker",
    "LatencyBenchmark",
    "ThroughputBenchmark",
    "MemoryProfiler",
    "AccuracyValidator",
    "ModelComparator",
    "benchmark_model",
    "compare_models",
    "BenchmarkConfig",
    "BenchmarkResult",
    "ComparisonResult",
    "BenchmarkMode",
]
